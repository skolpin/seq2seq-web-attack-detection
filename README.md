# Seq2seq Web Attack Detection
Макет обнаружения аномальных HTTP-запросов к веб-серверу.

## Гипотеза

Признаки веб-атак (SQL Injection, XSS, CSRF и др.) могут быть выявлены через обнаружение аномалий в HTTP-запросах.

## Способ

Рекуррентный нейросетевой автокодировщик типа «Sequence-to-Sequence» с LSTM, обученный на множестве нормальных HTTP-запросов, детектирует аномалии. Автокодировщик обучается воспроизводить с малой ошибкой входной запрос, промежуточно переводя его в скрытое представление меньшей размерности. При подаче аномального запроса на вход обученного автокодировщика ожидается большая ошибка воспроизведения, которая и позволит детектировать аномалию. В результате анализа аномального образца детектор также выделяет наиболее аномальные участки запроса.

## Данные

Набор нормальных и аномальных HTTP-запросов тестового веб-приложения [__train.txt__](/train.txt), [__anomaly.txt__](/anomaly.txt).

## Описание макета

HTTP-запрос в макете представляется как последовательность символов. Для кодирования символов создается словарь уникальных соответствий «символ – числовой код». Затем каждый символ в запросе кодируется по принципу «one-hot encoding», то есть представляется в виде вектора, длина которого равна объему словаря символов, и все элементы которого равны нулю, за исключением одного элемента, равного единице, стоящего на позиции кода этого символа. Запрос представляется в виде последовательности векторов, кодирующих символы.

Для обучения автокодировщика обучающие данные представляются в виде массива с размерностью [количество запросов в обучающей выборке] x [длина самого длинного запроса в обучающей выборке] x [объем словаря символов]. Так как запросы имеют разную длину, они дополняются до длины самого длинного запроса при помощи специального символа `<PAD>`, который также вносится в словарь и кодируется как прочие символы. Также вносятся в словарь и кодируются спецсимволы `<UNK>` (для будущего кодирования символов, не вошедших в словарь), `<GO>` (для кодирования начала запроса) и `<EOR>` (для кодирования конца запроса).

Модель Sequence-to-Sequence состоит из двух рекуррентных LSTM сетей — кодера и декодера. Кодер отображает входную последовательность в вектор скрытых состояний фиксированной длины. Декодер разворачивает скрытое представление в целевой вектор. При обучении автокодировщик представляет собой модель, в которой целевые значения устанавливаются такими же, как входные значения. 

Идея состоит в том, чтобы обучить автокодировщик декодировать нормальные запросы, или, другими словами, приближать их тождественное отображение. Если обученному автокодировщику подать на вход аномальный запрос, он, вероятно, воссоздает его с высокой степенью ошибки, просто потому, что никогда его не видел.

Схема автокодировщика Sequence-to-Sequence:

![seq2seq](/slides/seq2seq.png)

LSTM-блоки в модели отображают входной запрос в вектор скрытых состояний длиной 256. На выходе декодера расположен Softmax-слой с количеством нейронов, равным объему словаря символов. Особенность функции активации Softmax заключается в том, что сумма выходных значений всех нейронов в слое равна единице, а выход каждого нейрона можно интерпретировать как вероятность или уверенность модели в данном символе, что показано на рисунке как P(∙).

В результате обработки одного запроса на выходе модели получаем вектор вероятностей для каждого символа. На основе этого вектора вычисляется мера ошибки декодирования данного запроса:

$error=1-(\frac{1}{len(request)}\sum_{char\in request}P(char))$

Мера ошибки оперирует усреднением вероятностей по всем символам в запросе. Мотивация введения такой меры ошибки состоит в том, что, если модель качественно декодировала запрос, средняя вероятность по всем символам будет близка к единице, а значит мера ошибки будет близка к нулю. И наоборот, при подаче модели аномального запроса, она будет давать низкую вероятность для тех участков символов, которые не встречались в обучающих запросах и ошибка будет высокой.

После обучения автокодировщика на множестве нормальных запросов определяется пороговое значение ошибки для обнаружения аномальных запросов. Для этого из множества нормальных запросов выделяются валидационные нормальные запросы, не участвующие в обучении. После обучения модели на вход подаются валидационные запросы, для них вычисляется ошибка и определяется порог

$threshold=\widehat{m}(e)+k\widehat{\sigma}(e); k=5...6$

где $\widehat{m}(e)$ и $\widehat{\sigma}(e)$ – оценки математического ожидания и дисперсии ошибки для валидационных нормальных запросов.

После вычисления порога на вход модели подаются аномальные запросы, для них также вычисляется ошибка, которая сравнивается с порогом. При превышении порога запрос признается аномальным. Затем вычисленные значения ошибок для валидационных и аномальных данных, а также порог используются для построения гистограммы и вывода отчета о качестве обнаружения аномалий. Схема работы макета обнаружения аномальных HTTP-запросов представлена на рисунке:

![scheme](/slides/scheme.png)


Макет представляет из себя интерактивный блокнот Jupyter Noteboook __seq2seq-web-attack-detection.ipynb__ с программной реализацией на Python 3.6.8.

Для работы с ним необходимо установить Jupyter:

> pip install jupyter

после установки необходимо запустить сам Jupyter

> jupyter notebook

и открыть файл блокнота seq2seq-web-attack-detection.ipynb

Зависимости от сторонних библиотек и их версий перечислены в requirements.txt.
Для установки библиотек необходимо воспользоваться pip:

> pip install -r requirements.txt

К макету также прилагаются следующие файлы:

- utils.py - библиотека собственных функций
    
- train.txt, anomaly.txt - файлы с обучающими (нормальными) и тестовыми (аномальными) HTTP-запросами к веб-серверу
    
    
Референсы макета и источник данных:
- https://github.com/skolpin/seq2seq-web-attack-detection/blob/main/slides/detecting_web_attacks_rnn.pdf
- https://habr.com/ru/company/pt/blog/439202/


Сам макет seq2seq-web-attack-detection.ipynb при работе загружает из файлов train.txt и anomaly.txt нормальные и аномальные запросы, разбивает нормальные 
запросы на обучающие и валидационные (при этом количество валидационных запросов равно количеству аномальных), затем переводит все данные в 
вид трехмерных One-hot encoding массивов, строит и обучает модель рекуррентного seq2seq-автокодировщика и сохраняет ее в папку 
model. Изначально в этой папке уже есть обученная на этих данных модель.

Затем макет позволяет загрузить обученную модель с диска и обработать с помощью нее валидационные (=нормальные) и тестовые (=аномальные)
запросы. По результатам обработки валидационных запросов вычисляется порог ошибки реконструкции запроса, превышение которого признается 
аномалией.

Затем макет позволяет построить гистограмму распределения значений ошибки для нормальных и аномальных запросов относительно порога (сохраняя 
ее в файл "Histogram.png"), вычислить метрики качества обнаружения аномальных запросов  (сохраняя отчет в файл "test report.txt") и вывести 
ложные (FP и FN) срабатывания, а также несколько TP-срабатываний (истинно аномальных обнаруженных запросов) с выделением аномальных участков 
запроса.

Есть возможность пропустить этап обучения, если это уже было сделано ранее, 
и сразу перейти к загрузке модели с диска и их обработке. (все ячейки до обучения все равно необходимо выполнить)
